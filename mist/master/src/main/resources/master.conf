mist {

  work-directory = "."
  work-directory = ${?MIST_HOME}

  networking {
    host = "0.0.0.0"
    public-host = "auto"
  }

  cluster {
    host = ${mist.networking.host}
    public-host = ${mist.networking.public-host}
    port = 2551
  }

  http {
    host = ${mist.networking.host}
    public-host = ${mist.networking.public-host}
    port = 2004
    ui = ${mist.work-directory}"/ui"
    ui = ${?MIST_UI_DIR}
    ws-keepalive-tick = 1 minute // it must be less than akka.http.server.idle-timeout
  }

  mqtt {
    on = false
    host = "localhost"
    port = 1883
    publish-topic = ""
    subscribe-topic = ""
  }

  kafka {
    on = false
    host = "localhost"
    port = 9092
    publish-topic = ""
    subscribe-topic = ""
  }

  log-service {
    host = ${mist.networking.host}
    public-host = ${mist.networking.public-host}
    port = 2005
    dump_directory = ${mist.work-directory}"/logs"
  }

  db {
    // By default enabled legacy cofiguration support for the database access.
    // filepath = ${mist.work-directory}"/recovery.db"
    // Example of connection pooling to the H2 database same as filepath above
    driverClass = "org.h2.Driver"
    jdbcUrl = "jdbc:h2:file:"${mist.work-directory}"/recovery.db"
    username = ""
    password = ""
    poolSize = 1

    migration = on
  }

  workers {
    runner = "local"
    runner-init-timeout = 120 seconds
    ready-timeout = 120 seconds
    max-artifact-size = 250m

    docker {
      host = "unix:///var/run/docker.sock"
      image = ""
      mist-home = "/usr/share/mist"
      spark-home = "/usr/share/spark"
      network-type = "host" // bridge
      auto-master-network {
        container-id = ""
      }
    }

    manual {
      startCmd = ""
      stopCmd = null //optional
      async = true
    }

    // old manual options
    cmd = ""
    cmdStop = ""
  }

  context-defaults {
    downtime = 1 hour
    streaming-duration = 1 seconds
    max-parallel-jobs = 1
    max-jobs = 1
    precreated = false
    worker-mode = "exclusive" # shared | exclusive
    spark-conf {
      #spark.default.parallelism = 128
      #spark.driver.memory = "512m"
      #spark.executor.memory = "256m"
      #spark.scheduler.mode = "FAIR"
    }

    run-options = ""
    max-conn-failures = 5
  }

  context {}

  contexts-store {
    path = ${mist.work-directory}"/data/contexts"
  }
  functions-store {
    path = ${mist.work-directory}"/data/functions"
  }
  jobs-resolver {
    save-path = "/tmp"
  }
  artifact-repository {
    save-path = ${mist.work-directory}"/data/artifacts"
  }
  security {
    enabled = false
    keytab = ""
    principal = ""
    interval = 1 hour
  }

  job-extractor {
    init-timeout = 20s
    cache-entry-ttl = 3600s
    spark-conf {
      spark.driver.memory = "512m"
      spark.executor.memory = "256m"
    }
  }

}

writers-blocking-dispatcher {
  type = Dispatcher
  executor = "thread-pool-executor"
  thread-pool-executor {
    fixed-pool-size = 10
  }
  throughput = 1
}

akka {

  loggers = ["akka.event.slf4j.Slf4jLogger"]
  logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"

  jvm-exit-on-fatal-error = false
  loglevel = "INFO"
  actor {
    provider = "akka.remote.RemoteActorRefProvider"
    warn-about-java-serializer-usage = false
  }

  remote {
    netty.tcp {
      bind-hostname = ${mist.cluster.host}
      hostname = ${mist.cluster.public-host}
      port = ${mist.cluster.port}
      maximum-frame-size = 5242880b
    }
    transport-failure-detector {
      heartbeat-interval = 30s
      acceptable-heartbeat-pause = 5s
    }
    log-remote-lifecycle-events = off
  }

  http.server {
    idle-timeout = 10 minutes
    request-timeout = 2 minutes
    parsing.max-content-length = 250m
  }
}
